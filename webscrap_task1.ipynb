from lxml.html import fromstring
import requests
from itertools import cycle
import traceback
import requests
from bs4 import  BeautifulSoup
import json
stocks =['price','title','stock','maftr']
stocks_data = []

def get_proxies():
  url = 'https://free-proxy-list.net/'
  response = requests.get(url)
  parser = fromstring(response.text)
  proxies = set()
  for i in parser.xpath('//tbody/tr')[:10]:
    if i.xpath('.//td[7][contains(text(),"yes")]'):
      proxy = ":".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])
      proxies.add(proxy)
      return proxie
proxies = get_proxies()
proxy_pool = cycle(proxies)

url = 'https://www.midsouthshooterssupply.com/dept/reloading/primers?currentpage=1/'
for i in range(500,800):
#Get a proxy from the pool
   proxy = next(proxy_pool)
   print("Request #%d"%i)
   try:
      response = requests.get(url,proxies={"http": proxy, "https": proxy})
      print(response.json())
   except:
#Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. 
#We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url 
      print("Skipping. Connnection error")


#proxy ='201.217.245.229'
r = requests.get('https://www.midsouthshooterssupply.com/dept/reloading/primers?currentpage=1',proxies={'http':proxy,'htpps':proxy},timeout=2)
print(r.status_code)
#print(r.json())


def getData(symbol):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'}
    url = f'https://www.midsouthshooterssupply.com/dept/reloading/primers?currentpage=1/{symbol}'
    r = requests.get(url, headers=headers)
    soup = BeautifulSoup(r.text, 'html.parser')
    stocks = {
    'symbol': symbol, 
    'title': Soup.find('div',{'class':'product-heading'}).find_all('product-name').text,
    'price': Soup.find('div',{'class':'price'}).find_all('spain').text,
    'stock': Soup.find('div',{'class':'status'}).find_all('title').text,
    'maftr': Soup.find('div',{'class':'catalog-item-brand-item-number'}).find_all('spain').text
    }

 # def item_in_stock(r.text):
        
        #stock = soup.findAll("div", {"class": "status"})
       
        #if stock(out-of-stock):
            #print("false")
        #else:
            #print("true")*/



    return stocks


for item in stocks:
  stocks_data.append(getData(item))
  print('getting:',item)

with open('stocks_data.json','w') as f:
    json.dump(stocks_data,f)
print('final_data')
